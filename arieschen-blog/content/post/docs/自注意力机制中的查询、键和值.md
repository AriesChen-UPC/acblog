---
title: "自注意力机制中的查询、键和值"
description: ""
tags: [
    "Query",
    "Key",
    "Value",
    "Attention",
    "Self-Attention",
]
date: "2023-02-05"
categories: [
    "Natural Language Processing (NLP) & Large Language Models (LLM)",
]
author: "AriesChen"
menu:
  main:
    parent: "docs"
    weight: 3


---



## **1\. 引言：序列建模的挑战与注意力的兴起**

序列数据，如自然语言文本、时间序列信号等，在人工智能领域无处不在。有效处理和理解这些序列是许多任务（如机器翻译、文本摘要、情感分析）的核心。然而，传统的序列建模方法面临着固有的挑战。

### **1.1 传统序列模型（RNN）的局限性**

循环神经网络（Recurrent Neural Networks, RNNs）及其变体，如长短期记忆（Long Short-Term Memory, LSTM）和门控循环单元（Gated Recurrent Unit, GRU），曾是序列建模的主流方法 1。RNN 通过维护一个内部隐藏状态，按时间步顺序处理输入序列 3。这种顺序处理方式带来了几个关键限制：

1. **并行化困难**：RNN 的计算本质上是顺序的，当前时间步的计算依赖于前一时间步的隐藏状态 1。这种固有的顺序性阻碍了在训练样本内部进行并行计算，尤其是在处理长序列时，内存限制也会制约跨样本的批处理规模，导致训练效率低下 1。  
2. **长距离依赖问题**：理论上，RNN 的隐藏状态可以编码过去所有时间步的信息。然而在实践中，信息在序列中传播时，梯度可能会消失或爆炸（即“梯度消失/爆炸”问题） 6。这使得 RNN 难以捕捉序列中相距较远的元素之间的依赖关系 5。信息从序列的早期传递到后期时可能会衰减或失真 10。  
3. **固定长度上下文瓶颈**：在基本的编码器-解码器（Encoder-Decoder）架构中，编码器通常将整个输入序列压缩成一个固定长度的上下文向量（context vector）7。这种做法难以承载长序列的所有信息，成为模型性能的瓶颈 7。  
4. **信息偏向**：RNN 的结构倾向于更关注序列中较近期的信息，而早期信息的影响力可能随着处理步骤的增加而减弱 5。

### **1.2 注意力机制的引入**

为了缓解 RNN 的长距离依赖问题和固定上下文向量瓶颈，注意力机制（Attention Mechanism）应运而生 6。注意力机制的核心思想是允许模型在处理序列中的某个元素（例如，在解码器生成一个词时）时，能够“关注”输入序列中最相关的部分，而不是平等对待所有输入 6。它通过计算一组“注意力权重”（attention weights）来实现，这些权重反映了输入序列不同部分对于当前处理步骤的重要性 3。注意力机制最初被提出用于改进基于 RNN 的机器翻译模型，特别是处理长句子时效果显著 6。

### **1.3 自注意力：序列内部的关注**

自注意力（Self-Attention），有时也称为内部注意力（intra-attention），是注意力机制的一种特殊形式 5。与标准注意力机制通常关联输出序列元素和输入序列元素不同 7，自注意力机制关注的是 *同一个* 序列内部不同位置之间的关系，目的是计算该序列自身的表示 3。

自注意力机制由 Vaswani 等人在 2017 年的开创性论文《Attention Is All You Need》中提出，并作为 Transformer 模型的核心构件 1。Transformer 架构完全摒弃了 RNN 的循环结构和 CNN 的卷积结构，仅依赖注意力机制来捕捉输入和输出之间的全局依赖关系 1。

### **1.4 本报告焦点：解构查询、键、值机制**

Transformer 及其核心自注意力机制的成功，很大程度上归功于其内部精巧的设计，特别是查询（Query, Q）、键（Key, K）和值（Value, V）这三个核心组件。理解 QKV 机制是理解 Transformer 如何有效捕捉上下文关系、处理长距离依赖以及实现高效并行计算的关键 3。本报告旨在深入剖析自注意力机制中 QKV 的概念、作用、计算过程及其设计原理，阐明为何采用这种特定结构以及它如何赋予模型强大的序列处理能力。

## **2\. 理解自注意力机制**

### **2.1 定义与目标**

自注意力机制的核心目标是为输入序列中的每个元素（例如，句子中的每个单词或标记）生成一个上下文感知的表示（context-aware representation）6。它通过计算序列中所有元素相对于当前元素的重要性权重，然后基于这些权重聚合信息来实现 3。

形式上，给定一个输入序列的向量表示 X=(x1​,x2​,...,xn​)，其中 xi​ 是第 i 个元素的向量（通常是词嵌入加上位置编码），自注意力机制的目标是计算一个输出序列 Z=(z1​,z2​,...,zn​)。每个输出向量 zi​ 都是通过对输入序列中所有元素的某种形式的信息进行加权求和得到的，权重取决于 xi​ 与 xj​ (对于所有 j) 之间的关系 29。这个过程使得 zi​ 不仅包含 xi​ 自身的信息，还融入了来自整个序列的上下文信息 6。

自注意力机制可以被视为一种动态构建图结构的过程。如果将输入序列的每个元素看作图中的一个节点，那么自注意力机制就在这些节点之间动态地计算边的权重（即注意力分数）。这些权重并非预先固定，而是根据节点的内容（元素表示）以及它们之间的相互关系，通过学习得到的参数（如 QKV 的投影矩阵）实时计算得出 3。这种方式使得模型能够为每个输入序列构建一个独特的、完全连接的加权图，信息可以根据学习到的关系灵活地在节点间流动。这与卷积神经网络（CNN）中固定的局部连接模式或 RNN 中严格的顺序信息流形成了鲜明对比。

### **2.2 在 Transformer 架构中的角色**

自注意力层是 Transformer 模型编码器和解码器堆栈的基本构建模块 1。

* **编码器（Encoder）**：在编码器中，每个自注意力层允许输入序列中的每个位置关注（attend to）序列中的所有其他位置（包括自身）8。这使得编码器能够构建包含全局上下文信息的输入表示。编码器通常由 N 个相同的层堆叠而成，每层包含一个多头自注意力子层和一个位置前馈网络子层 9。  
* **解码器（Decoder）**：解码器同样包含自注意力层和前馈网络层，但还额外包含一个用于关注编码器输出的注意力层 9。解码器中的自注意力层通常是“掩码”（masked）自注意力层 9。这种掩码确保在预测位置 i 的输出时，只能关注到位置 i 及其之前的位置，防止模型“看到”未来的信息，这对于自回归生成任务（如翻译或文本生成）至关重要。解码器中的第二个注意力层，通常称为编码器-解码器注意力（Encoder-Decoder Attention）或交叉注意力（Cross-Attention），允许解码器的每个位置关注编码器的所有输出位置 5。在这个层中，查询（Q）来自前一个解码器层，而键（K）和值（V）来自编码器的输出。

本报告主要聚焦于自注意力变体，即 Q、K、V 均源自同一输入序列的情况。

### **2.3 与标准注意力的关键区别**

自注意力与早期（如 Bahdanau 或 Luong 注意力）在 RNN 编码器-解码器模型中使用的标准注意力机制的主要区别在于 Q、K、V 的来源 10。在自注意力中，查询（Q）、键（K）和值（V）这三个向量都是从 *同一个* 输入序列（例如，编码器的输入序列或解码器的输入序列）通过不同的线性变换派生出来的 7。这使得输入序列的元素能够相互交互，计算彼此之间的注意力权重 7。

相比之下，在典型的 RNN 编码器-解码器注意力中，查询通常来自解码器的当前隐藏状态，而键和值则来自编码器的所有隐藏状态 10。这种设置使得解码器能够关注输入序列的不同部分。

## **3\. 查询、键、值（QKV）抽象**

自注意力机制的核心是查询（Query）、键（Key）和值（Value）这三个向量。理解它们的角色和相互作用对于掌握自注意力的工作原理至关重要。

### **3.1 定义角色**

对于输入序列中的每个元素（token），我们通过特定的变换生成三个向量：Q、K 和 V。它们的概念性作用如下 4：

* **查询（Query, Q）**：代表当前正在处理的元素（token）的视角。它像一个探针或问题 6，用于探索序列中的其他元素，以确定哪些元素与当前元素最相关，从而更好地理解当前元素的上下文。查询向量表达了当前元素“想要了解什么”或“正在寻找什么” 5。  
* **键（Key, K）**：代表序列中（包括当前元素自身在内）的每个元素所携带的一种“标签”或“标识符” 5。它描述了该元素“能够提供什么信息”或“代表什么内容”，用于和查询向量进行匹配。查询向量会与所有键向量进行比较，以评估相关性 6。  
* **值（Value, V）**：代表序列中每个元素的实际内容或信息 5。一旦通过查询和键的比较确定了相关性（注意力权重），这些权重就会作用于对应的值向量。最终的输出是这些加权值向量的聚合，表示模型根据相关性“提取”到的信息。

### **3.2 信息检索类比**

理解 QKV 概念的一个常用且有效的方法是将其类比于信息检索系统，例如数据库查询、字典查找或搜索引擎 6：

* **查询（Query）**：相当于你在搜索引擎中输入的搜索词 22，或者你想在字典里查找的单词，或是数据库的查询请求。  
* **键（Key）**：相当于搜索引擎索引中的网页标题或关键词，字典中的词条，或者数据库中用于匹配查询的索引字段 13。系统会将你的查询与这些键进行比较，以找到匹配项。  
* **值（Value）**：相当于搜索结果返回的网页内容，字典中单词的释义，或者数据库中与键对应的完整记录 13。

在自注意力中，每个元素都扮演着查询者的角色，向序列中的所有其他元素（作为键值对）“提问”。通过比较查询和键，计算出每个键（即每个其他元素）与当前查询的相关程度（相似度得分）。然后，这些得分被用来对相应的值（即其他元素所包含的信息）进行加权求和，最终得到一个融合了来自所有相关元素信息的、上下文感知的表示 6。

### **3.3 Q, K, V 的生成：学习的线性投影**

需要强调的是，Q、K、V 向量并非直接使用输入元素的嵌入向量 xi​，而是通过将 xi​ 与三个不同的、在训练过程中学习到的权重矩阵 WQ、WK 和 WV 相乘得到的 4：

qi​=xi​WQ  
ki​=xi​WK  
vi​=xi​WV  
这些权重矩阵 WQ,WK,WV 是模型的可训练参数，它们通过反向传播算法在训练数据上进行学习 4。

在维度方面，查询向量 qi​ 和键向量 ki​ 的维度必须相同，记为 dk​。值向量 vi​ 的维度可以不同，记为 dv​ 6。在原始 Transformer 论文中，特别是在多头注意力（Multi-Head Attention）的设置下，通常取 dk​=dv​=dmodel​/h，其中 dmodel​ 是模型内部的表示维度（例如 512），h 是注意力头的数量（例如 8）26。

信息检索的类比虽然直观，但可能未能完全体现 QKV 机制的精妙之处。关键在于 Q、K、V 是通过 *学习到的* 线性投影从 *相同* 的输入嵌入生成的 6。这些投影矩阵是模型端到端训练的一部分，这意味着模型会根据具体的任务和数据，自主学习如何将输入嵌入转换成最有效的查询、键和值表示 6。模型学习到的不仅仅是简单的匹配，而是如何为查询、匹配和信息传递这三个不同角色提取输入元素中最相关的语义或句法特征 6。因此，自注意力更像是一个“软性的”、可学习的字典或检索系统，其内部的“索引”（键）和“查询方式”（查询）以及“返回内容”（值）都是为特定上下文表示任务量身定制和优化的 22。一些观点甚至认为，由于 Q、K、V 向量在经过多层 Transformer 后其表示已经与初始嵌入大相径庭，将其简单地视为原始单词之间的比较可能过于简化，它更像是一个通用的计算单元，用于高效地提取和整合信息 35。

## **4\. 缩放点积注意力的计算机制**

Transformer 中使用的自注意力机制的具体实现形式被称为“缩放点积注意力”（Scaled Dot-Product Attention）。其计算过程可以分解为以下三个主要步骤：

### **4.1 步骤 1：衡量相关性 \- 查询与键的点积相似度**

计算的核心是衡量查询向量与所有键向量之间的相似度或相关性 4。对于当前处理的标记 i 的查询向量 qi​ 和序列中任意标记 j 的键向量 kj​，它们之间的相似度得分通常通过计算它们的点积（dot product）得到 4：

score(qi​,kj​)=qi​⋅kj​=qiT​kj​

点积值越大，通常表示 qi​ 和 kj​ 之间的相似度或相关性越高 4。

在实践中，为了高效计算，通常将所有查询向量、键向量和值向量分别打包成矩阵 Q、K 和 V。其中 Q 的维度是 n×dk​，K 的维度是 n×dk​，V 的维度是 n×dv​，n 是序列长度。所有查询和键之间的相似度得分可以通过一次矩阵乘法计算得到 2：

Scores=QKT

生成的 Scores 矩阵维度为 n×n，其中第 (i,j) 个元素 Scoresij​ 表示查询 qi​ 和键 kj​ 之间的原始（未缩放的）注意力得分。

### **4.2 步骤 2：稳定与归一化 \- 缩放因子 (dk​​) 与 Softmax**

在获得原始的点积得分后，需要进行两项处理：缩放和 Softmax 归一化。

* 缩放（Scaling）：将 Scores 矩阵中的所有元素除以一个缩放因子 dk​​，其中 dk​ 是键向量（也是查询向量）的维度 2。  
  ScaledScores=dk​​QKT​  
  **缩放的原因**：这一步对于稳定训练过程至关重要。当 dk​ 的维度较大时，点积 qiT​kj​ 的结果的方差也会随之增大（假设 qi​ 和 kj​ 的分量是均值为 0、方差为 1 的独立随机变量，则点积的均值为 0，方差为 dk​）43。过大的点积值会使得 Softmax 函数的梯度变得非常小（接近饱和区），这会严重减慢或阻碍模型的学习过程（梯度消失问题）6。通过除以 dk​​，可以将点积的方差稳定在 1 左右，使得 Softmax 函数在其输入变化时更加敏感，从而有助于更有效的梯度传播和模型收敛 6。  
* Softmax 归一化：将缩放后的得分矩阵 ScaledScores 逐行（row-wise）通过 Softmax 函数进行归一化 2。  
  Weights=softmax(dk​​QKT​)  
  Softmax 函数将每一行的得分转换成一个概率分布，使得该行所有元素的和为 1，且每个元素值都在 0 到 1 之间 4。Weights 矩阵中的元素 αij​ 表示查询 qi​ 对键 kj​（以及对应的值 vj​）分配的注意力权重。权重越大，表示 vj​ 对计算 zi​ 的贡献越大。  
  （可选说明：在解码器的掩码自注意力中，会在应用 Softmax 之前，将对应于未来位置的缩放得分设置为一个非常大的负数（例如 −∞ 或 \-1e9），这样 Softmax 的输出会接近于 0，从而阻止信息从未来位置流向当前位置 5。）

### **4.3 步骤 3：计算输出 \- 值的加权求和**

最后一步是利用计算出的注意力权重 Weights 来对值向量 V 进行加权求和，得到最终的输出序列 Z 5。

对于每个输出位置 i，其对应的输出向量 zi​ 是所有值向量 vj​ 的加权和，权重为 αij​ 5：

zi​=∑j=1n​αij​vj​

用矩阵形式表示，整个输出序列 Z 可以通过将权重矩阵 Weights 与值矩阵 V 相乘得到 2：

Z=Weights⋅V=softmax(dk​​QKT​)V

得到的输出向量 zi​ 就是输入元素 xi​ 经过自注意力机制处理后的、包含了来自整个序列上下文信息的表示 6。Z 矩阵的维度是 n×dv​。

### **4.4 表格：缩放点积注意力公式总结**

为了清晰地展示缩放点积注意力的核心计算过程，下表总结了其主要组成部分和整体公式：

| 组件 | 符号 | 描述 | 引用片段 |
| :---- | :---- | :---- | :---- |
| 查询 (Queries) | Q | 查询向量矩阵 (每个输入 token 一个)，由输入嵌入经 WQ 变换得到。维度 n×dk​。 | 6 |
| 键 (Keys) | K | 键向量矩阵 (每个输入 token 一个)，由输入嵌入经 WK 变换得到。维度 n×dk​。 | 6 |
| 值 (Values) | V | 值向量矩阵 (每个输入 token 一个)，由输入嵌入经 WV 变换得到。维度 n×dv​。 | 6 |
| 键维度 | dk​ | 键向量 (及查询向量) 的维度。 | 2 |
| 相似度得分 | QKT | 所有查询和键之间的点积矩阵。维度 n×n。 | 2 |
| 缩放后得分 | dk​​QKT​ | 经键维度平方根缩放后的相似度得分。 | 2 |
| 注意力权重 | softmax(⋅) | 经 Softmax 函数逐行归一化后的缩放得分，形成概率分布。维度 n×n。 | 2 |
| 输出 | Z | 最终的上下文感知输出矩阵，由值的加权和计算得到。维度 n×dv​。 | 2 |
| **整体公式** |  | Attention(Q,K,V)=softmax(dk​​QKT​)V | 2 |

## **5\. 为何采用查询、键、值？设计原理探讨**

自注意力机制采用 QKV 结构并非偶然，这种设计背后蕴含着深刻的原理，赋予了模型强大的能力和灵活性。

### **5.1 实现动态重要性加权**

自注意力的核心功能是动态地确定输入序列中不同部分相对于某个特定位置（由查询 Q 代表）的重要性 3。QKV 机制正是实现这一目标的关键：

* **查询-键 (QK) 相似度计算**：通过计算查询 qi​ 与所有键 kj​ 的相似度（通常是点积），模型可以评估序列中每个元素 j 对于当前元素 i 的上下文理解有多大关联 3。  
* **注意力权重**：Softmax 函数将这些相似度得分转化为归一化的注意力权重 αij​。这些权重直接反映了元素 j 的信息（由 vj​ 承载）对于构建元素 i 的新表示 zi​ 的重要程度。  
* **加权聚合**：最终的输出 zi​ 是所有值向量 vj​ 基于注意力权重 αij​ 的加权和。这意味着模型能够有效地“聚焦”于那些被判断为高度相关的元素所携带的信息，同时“忽略”或降低不太相关元素的影响 3。

重要的是，这种重要性权重是 **动态** 和 **上下文相关** 的。同一个词在不同的句子中，或者在同一个句子中作为不同词的上下文时，其计算出的 Q、K、V 向量以及与其他词的注意力权重都可能不同 8。QKV 机制使得这种依赖于具体内容的动态加权成为可能。

### **5.2 灵活性与表示能力**

将输入嵌入 xi​ 通过 *不同* 的线性投影矩阵 WQ,WK,WV 转换成 Q、K、V，而不是直接使用 xi​ 或单一变换，是 QKV 设计的另一个关键优势，它带来了显著的灵活性和更强的表示能力 6。

* **专门化角色表示**：使用独立的 WQ,WK,WV 允许模型为 Q、K、V 这三个不同的角色学习到专门化的表示 6。例如，模型可以学习到：  
  * WQ 提取输入 xi​ 中最适合用于“提问”或“探询”上下文信息的特征。  
  * WK 提取 xi​ 中最适合用于“被匹配”或“标识”自身信息类型的特征。  
  * WV 提取 xi​ 中最适合作为“内容”被传递和聚合的特征。 这种分离使得模型能够更精细地控制注意力过程的各个方面。  
* **增强模型容量**：三个独立的权重矩阵引入了更多的可训练参数，增加了模型的容量，使其能够学习和表达输入序列中更复杂、更细微的模式和关系 6。  
* **非对称关系建模**：如果 Q、K、V 来自同一个向量（例如都等于 xi​），那么点积相似度 xiT​xj​ 是对称的，且 xi​ 与自身的相似度可能总是最高的。使用不同的 WQ 和 WK 使得相似度计算 qiT​kj​=(xi​WQ)T(xj​WK) 可以是非对称的。这使得模型能够更容易地学习非对称的关系，例如一个代词（作为 Q）应该强烈关注其指代的名词（作为 K），即使它们在原始嵌入空间中的表示不一定最接近 29。  
* **多头注意力的基础**：QKV 的分离设计是多头注意力（Multi-Head Attention）机制的基础 1。在多头注意力中，模型并行地学习 h 组不同的 WiQ​,WiK​,WiV​ 矩阵（i=1...h）。每一组（称为一个“头”）可以专注于输入数据的不同方面或不同类型的关系（例如，关注句法关系、语义关系、或者不同距离的依赖关系）。每个头独立地计算其注意力输出，然后将所有头的输出拼接（concatenate）并再次进行线性变换，得到最终的多头注意力输出。这种机制极大地增强了模型从不同表示子空间捕捉信息的能力。没有 QKV 的分离，实现这种多角度、多子空间的关注会困难得多。

将 Q、K、V 通过不同的学习矩阵进行分离，使得自注意力机制能够内在地学习到与任务相关的关系偏好。由于 WQ,WK,WV 是与模型其他部分一起根据最终任务目标（如翻译、摘要）进行端到端训练的 6，模型会学习到那些对于解决该任务最有效的转换方式。例如，一个用于机器翻译的模型可能会学习到能够突出句法依赖关系的投影，而一个情感分析模型可能会学习到更能区分情感极性词汇的投影。这种分离允许模型独立地优化“如何提问”（Q）、“如何被识别”（K）以及“传递什么信息”（V）这三个方面，以最好地服务于下游任务，而不是依赖于单一的、可能不够灵活的表示。

## **6\. 捕捉长距离依赖与上下文**

自注意力机制，特别是其 QKV 结构，在捕捉序列中长距离依赖关系和构建全局上下文方面，相比传统 RNN 具有显著优势。

### **6.1 克服 RNN 的路径长度限制**

RNN 处理序列时，信息需要在时间步之间顺序传递 5。这意味着相距遥远的两个词之间的依赖关系，信息必须经过中间所有时间步的传递，路径长度为 O(n)（n 为距离）。在这个过程中，信息可能会因为梯度消失或重复的状态更新而衰减或失真，使得 RNN 难以有效学习长距离依赖 6。

相比之下，自注意力机制允许序列中的任意两个位置直接进行交互 1。查询 qi​ 可以直接与序列中任何位置 j 的键 kj​ 计算点积相似度，无论 i 和 j 之间的距离有多远。从计算角度看，任意两个位置之间的“路径长度”缩短为 O(1) 1。这种直接的、不考虑距离的交互能力使得自注意力机制能够非常有效地捕捉长距离依赖关系 1，这对于理解复杂的语言现象（如长句中的指代消解、跨从句的主谓一致等）至关重要。

### **6.2 通过 QKV 实现直接的成对交互**

QKV 机制是实现这种直接交互的核心。每个标记 i 生成的查询向量 qi​ 作为一个探针，直接与序列中所有其他标记 j 的键向量 kj​ 进行比较，以评估它们之间的相关性 22。基于这些相关性得分，模型随后聚合来自所有标记的值向量 vj​。这个过程对序列中的每一对 (i,j) 都（隐式地）进行了计算，允许模型同时考虑所有词之间的相互影响，从而构建一个全局的、丰富的上下文表示 3。

自注意力层内部 O(1) 的交互路径长度不仅仅是为了直接捕捉远距离依赖，它对于有效堆叠多层注意力网络也至关重要。RNN 由于 O(n) 的路径长度和梯度问题，很难有效地构建非常深的网络 6。而自注意力机制由于信息在单层内传递时损耗较小，使得 Transformer 模型可以堆叠很多层（例如，BERT-Large 有 24 层）9。每一层的输出 Z 已经融合了来自整个序列的上下文信息，当它作为下一层的输入 X′ 并再次计算 Q', K', V' 时，模型就能学习到更高阶、更抽象的关系——即在已经经过上下文编码的表示之间建立依赖关系。这种逐层深入的上下文构建能力，使得 Transformer 能够学习到类似于语法树结构或者抽象语义关系的复杂模式 37，这是 RNN 难以企及的。

## **7\. 自注意力（QKV）与循环处理（RNN）的比较**

将基于 QKV 的自注意力机制与传统的 RNN 进行比较，可以更清晰地看到其优势所在。

### **7.1 计算复杂度与并行化**

* **自注意力**：每一层的计算复杂度通常被认为是 O(n2⋅d)，其中 n 是序列长度，d 是表示维度 46。这个复杂度主要来自于计算 n×n 的注意力得分矩阵 QKT。如果包含初始的 QKV 线性投影（将 n×d 的输入映射到 n×d 的 Q, K, V），则总复杂度为 O(n2⋅d+n⋅d2) 39。虽然复杂度在序列长度 n 上是二次的，但关键在于，对于给定序列，所有位置的计算（Q, K, V 的生成、点积得分、加权求和）可以大规模并行执行，因为它们不依赖于前一位置的计算结果 1。  
* **RNN**：RNN 每处理一个时间步的复杂度大约是 O(d2)（取决于具体的 RNN 单元），处理整个长度为 n 的序列需要 O(n⋅d2) 的总计算量。然而，这些计算必须按顺序执行，因为每个时间步都依赖于前一个时间步的状态 1。这严重限制了跨时间步的并行化 1。  
* **比较**：尽管自注意力在 n 上有二次复杂度，但其高度的并行性使其在现代并行计算硬件（如 GPU 和 TPU）上训练速度通常远快于 RNN，特别是当序列长度 n 不超过表示维度 d 时（原始论文观点 39，尽管实际情况可能更复杂）。RNN 则受限于其固有的顺序计算瓶颈 1。

### **7.2 依赖关系处理（路径长度）**

* **自注意力**：如前所述，任意两个位置之间的最大信息传递路径长度在单层内是 O(1) 1。这使得模型能够轻松地学习和利用长距离依赖关系 1。QKV 机制通过允许任何查询 qi​ 直接与任何键 kj​ 交互来实现这一点。  
* **RNN**：信息必须沿着序列逐步传递，最大路径长度为 O(n) 6。这使得捕捉远距离依赖关系变得困难。  
* **QKV 优势**：QKV 结构是实现 O(1) 有效路径长度的关键，从而克服了 RNN 在处理长距离依赖方面的根本性限制。

### **7.3 上下文表示质量**

* **自注意力**：通过 QKV 交互和加权聚合，自注意力能够动态地整合来自整个序列的信息，构建出高度上下文相关的表示 5。它能更好地根据全局上下文来区分多义词的不同含义 5。  
* **RNN**：上下文信息是按顺序逐步积累的，可能会丢失早期信息，或者受限于固定大小的隐藏状态或最终的上下文向量 5。  
* **QKV 优势**：QKV 机制以及学习到的投影矩阵赋予了自注意力极大的灵活性，使其能够捕捉比 RNN 的顺序聚合方式更丰富、更细致的上下文关系。

### **7.4 表格：自注意力 (QKV) 与 RNN 对比总结**

下表总结了基于 QKV 的自注意力与 RNN 在关键特性上的对比，突显了 QKV 设计带来的优势：

| 特性 | 自注意力 (含 QKV) | 循环神经网络 (RNN) | QKV 设计优势 | 引用片段 |
| :---- | :---- | :---- | :---- | :---- |
| **并行化能力** | 高 (各位置计算相对独立) | 低 (跨时间步必须顺序执行) | QKV 允许同时计算所有交互，极大提升了在并行硬件上的训练效率。 | 1 |
| **依赖路径长度** | O(1) (直接成对交互) | O(n) (信息顺序传递) | QKV 实现了任意 token 间的直接比较，易于捕捉长距离依赖。 | 1 |
| **计算复杂度 (每层)** | O(n2⋅d) 或 O(n2⋅d+n⋅d2) | O(n⋅d2) (但受顺序执行限制) | 尽管 n2 项存在，但并行性使其在实践中（尤其 GPU 上）通常更快。QKV 是此并行计算的基础。 | 1 |
| **上下文处理** | 基于 QKV 相关性的全局动态加权 | 顺序积累，可能存在信息丢失 | QKV 允许根据学习到的相关性动态整合全局上下文，表示更丰富。 | 5 |
| **灵活性/表示力** | 高 (学习的 QKV 投影适应任务/上下文) | 较低 (固定的状态转移函数) | 独立的 QKV 投影提供了强大的表示能力和灵活性，以建模多样化的关系。 | 6 |

## **8\. 结论：QKV 机制的重要性**

查询（Q）、键（K）、值（V）的抽象是自注意力机制的核心，也是 Transformer 架构取得巨大成功的关键因素之一。

### **8.1 QKV 功能与效果回顾**

总结来说，QKV 机制通过以下方式运作并发挥作用：

* **学习的投影**：通过独立的权重矩阵 WQ,WK,WV 将输入嵌入投影到不同的表示空间，为查询、匹配和内容传递提供了灵活性和专门化。  
* **相关性度量**：查询和键的点积计算提供了一种有效的方式来衡量序列中任意两个元素之间的相关性。  
* **稳定与归一化**：dk​​ 缩放因子稳定了训练过程，Softmax 函数将相关性得分转换为可解释的注意力权重分布。  
* **动态加权聚合**：利用注意力权重对值向量进行加权求和，生成了融合了全局上下文信息的、动态调整的输出表示。

这种机制使得模型能够根据输入内容动态地聚焦于最重要的信息，有效地处理上下文依赖关系。

### **8.2 对现代 NLP 和 Transformer 的影响**

基于 QKV 的自注意力机制是 Transformer 架构的基石 1。通过克服 RNN 在并行计算和长距离依赖方面的限制，它引发了自然语言处理领域的革命。几乎所有当前最先进的大型语言模型（LLM），如 BERT、GPT 系列等，都以 Transformer 架构为基础 3。这些模型在机器翻译、文本摘要、问答系统、文本生成等众多 NLP 任务上取得了前所未有的成果 3。此外，自注意力和 Transformer 的思想也成功地扩展到了计算机视觉、语音处理等其他领域 15。

### **8.3 更广泛的启示与未来方向**

QKV 自注意力机制的成功，不仅仅是 NLP 技术的进步，它也揭示了一种强大的、通用的计算范式。可以将 QKV 看作是一种基于内容的、可学习的信息检索与聚合原语 22。其核心思想——根据一个表示（Q vs K）比较元素，并根据另一个表示（V）聚合信息——具有广泛的适用性。只要任务需要根据上下文动态地加权和路由信息，这种机制就有可能发挥作用。它在视觉、图数据、强化学习等领域的应用探索也证明了其通用性 15。

当然，自注意力机制也面临挑战并持续发展。例如，其 O(n2) 的计算复杂度限制了其在处理超长序列时的效率，催生了许多旨在降低复杂度的研究（如稀疏注意力、线性注意力等）38。提高注意力的可解释性 46 以及探索注意力机制的替代或补充方案 49 也是当前活跃的研究方向。

总而言之，查询、键、值的精巧设计是自注意力机制的核心，它赋予了模型前所未有的处理序列数据、捕捉上下文和长距离依赖的能力，并为现代人工智能的发展奠定了重要基础。对 QKV 机制的深入理解，对于理解和推进当前 AI 技术至关重要。
